{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQuIzzuPMa-t",
        "outputId": "c6c636bb-d18d-47b6-a5d5-55156ef32493"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting lamini\n",
            "  Downloading lamini-3.1.3-12-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting lamini-configuration[yaml] (from lamini)\n",
            "  Downloading lamini_configuration-0.8.3-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from lamini) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from lamini) (4.66.6)\n",
            "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (from lamini) (1.26.4)\n",
            "Collecting jsonlines (from lamini)\n",
            "  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from lamini) (2.2.2)\n",
            "Collecting azure-storage-blob (from lamini)\n",
            "  Downloading azure_storage_blob-12.24.0-py3-none-any.whl.metadata (26 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from lamini) (1.5.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from lamini) (3.11.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->lamini) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->lamini) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->lamini) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->lamini) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->lamini) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->lamini) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->lamini) (1.17.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->lamini) (4.0.3)\n",
            "Collecting azure-core>=1.30.0 (from azure-storage-blob->lamini)\n",
            "  Downloading azure_core-1.32.0-py3-none-any.whl.metadata (39 kB)\n",
            "Requirement already satisfied: cryptography>=2.1.4 in /usr/local/lib/python3.10/dist-packages (from azure-storage-blob->lamini) (43.0.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from azure-storage-blob->lamini) (4.12.2)\n",
            "Collecting isodate>=0.6.1 (from azure-storage-blob->lamini)\n",
            "  Downloading isodate-0.7.2-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from lamini-configuration[yaml]->lamini) (6.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->lamini) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->lamini) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->lamini) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->lamini) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->lamini) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->lamini) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->lamini) (2024.8.30)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->lamini) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->lamini) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->lamini) (3.5.0)\n",
            "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from azure-core>=1.30.0->azure-storage-blob->lamini) (1.16.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=2.1.4->azure-storage-blob->lamini) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=2.1.4->azure-storage-blob->lamini) (2.22)\n",
            "Downloading lamini-3.1.3-12-py3-none-any.whl (693 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m694.0/694.0 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_storage_blob-12.24.0-py3-none-any.whl (408 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m408.6/408.6 kB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
            "Downloading azure_core-1.32.0-py3-none-any.whl (198 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m198.9/198.9 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading isodate-0.7.2-py3-none-any.whl (22 kB)\n",
            "Downloading lamini_configuration-0.8.3-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: lamini-configuration, jsonlines, isodate, azure-core, azure-storage-blob, lamini\n",
            "Successfully installed azure-core-1.32.0 azure-storage-blob-12.24.0 isodate-0.7.2 jsonlines-4.0.0 lamini-3.1.3 lamini-configuration-0.8.3\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement util (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for util\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install lamini\n",
        "!pip install util"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import lamini\n",
        "lamini.api_key = \"9bfb9cf42602648f7ea46db4d2405b0a33f858ca07ce39868b862811919866d9\"\n",
        "\n",
        "llm = lamini.Lamini(\"meta-llama/Meta-Llama-3.1-8B-Instruct\")\n",
        "print(llm.generate(\"\"\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "how llama3 is difer from gpt?<|eot_id|>\n",
        "<|start_header_id|>assistant<|end_header_id|>\n",
        "\n",
        "\"\"\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wzRe_nE2Nw_e",
        "outputId": "8a8d7151-5df1-462c-f16a-f9d34d782717"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You're asking about the differences between LLaMA and GPT!\n",
            "\n",
            "LLaMA and GPT are both large language models developed by Meta AI, but they have some key differences:\n",
            "\n",
            "**1. Training data:**\n",
            "\t* GPT (Generative Pre-trained Transformer) was trained on a massive dataset of text from the internet, specifically from the Common Crawl dataset, which includes a wide range of texts from the web.\n",
            "\t* LLaMA (Large Language Model Meta AI) was trained on a dataset that is similar to GPT's, but with a focus on more conversational and dialogue-based data, as well as a larger portion of text from the internet.\n",
            "\n",
            "**2. Architecture:**\n",
            "\t* GPT is based on the transformer architecture, which is a type of neural network designed for natural language processing tasks.\n",
            "\t* LLaMA is also based on the transformer architecture, but with some modifications to improve its performance on conversational tasks.\n",
            "\n",
            "**3. Size and complexity:**\n",
            "\t* GPT-3 has 175 billion parameters, making it one of the largest language models in the world.\n",
            "\t* LLaMA has 40 billion parameters, which is significantly smaller than GPT-3.\n",
            "\n",
            "**4. Training objectives:**\n",
            "\t* GPT was trained to predict the next word in a sequence, which is a common objective for language models.\n",
            "\t* LLaMA was trained to generate coherent and contextually relevant text, with a focus on conversational dialogue.\n",
            "\n",
            "**5. Evaluation metrics:**\n",
            "\t* GPT has been evaluated on a range of tasks, including language translation, text summarization, and question-answering.\n",
            "\t* LLaMA has been evaluated on conversational tasks, such as dialogue generation and response generation.\n",
            "\n",
            "**6. Availability:**\n",
            "\t* GPT-3 is available as a cloud API, allowing developers to integrate it into their applications.\n",
            "\t* LLaMA is not yet available as a cloud API, but it is available for research purposes through the Meta AI Research website.\n",
            "\n",
            "In summary, while both LLaMA and GPT are large language models, they have different training objectives, architectures, and evaluation metrics. LLaMA is designed to excel at conversational tasks, while GPT is a more general-purpose language model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import sqlite3\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "7wJNJi_oOf0z"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logger = logging.getLogger(__name__)\n",
        "engine = sqlite3.connect(\"./nba_roster.db\")"
      ],
      "metadata": {
        "id": "2W7ITCsQjFEp"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = lamini.Lamini(model_name=\"meta-llama/Meta-Llama-3-8B-Instruct\")"
      ],
      "metadata": {
        "id": "tFDlpU-yi63O"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Meta Llama 3 Instruct uses a prompt template, with special tags used to indicate the user query and system prompt.\n",
        "# You can find the documentation on this [model card](https://llama.meta.com/docs/model-cards-and-prompt-formats/meta-llama-3/#meta-llama-3-instruct).\n",
        "def make_llama_3_prompt(user, system=\"\"):\n",
        "    system_prompt = \"\"\n",
        "    if system != \"\":\n",
        "        system_prompt = (\n",
        "            f\"<|start_header_id|>system<|end_header_id|>\\n\\n{system}<|eot_id|>\"\n",
        "        )\n",
        "    return f\"<|begin_of_text|>{system_prompt}<|start_header_id|>user<|end_header_id|>\\n\\n{user}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\""
      ],
      "metadata": {
        "id": "6MxSa_RnhSuc"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_schema():\n",
        "    return \"\"\"\\\n",
        "0|Team|TEXT\n",
        "1|NAME|TEXT\n",
        "2|Jersey|TEXT\n",
        "3|POS|TEXT\n",
        "4|AGE|INT\n",
        "5|HT|TEXT\n",
        "6|WT|TEXT\n",
        "7|COLLEGE|TEXT\n",
        "8|SALARY|TEXT eg.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "LpC5EECcimYc"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user = \"\"\"Who is the highest paid NBA player?\"\"\""
      ],
      "metadata": {
        "id": "FqETZPmijccd"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "system = f\"\"\"You are an NBA analyst with 15 years of experience writing complex SQL queries. Consider the nba_roster table with the following schema:\n",
        "{get_schema()}\n",
        "\n",
        "Write a sqlite query to answer the following question. Follow instructions exactly\"\"\""
      ],
      "metadata": {
        "id": "Gvj1erGhi-UW"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(system)"
      ],
      "metadata": {
        "id": "QIlSzIyBjNxL",
        "outputId": "f66d1225-15f4-4cd9-e5cc-441d5bd97c7c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are an NBA analyst with 15 years of experience writing complex SQL queries. Consider the nba_roster table with the following schema:\n",
            "0|Team|TEXT \n",
            "1|NAME|TEXT  \n",
            "2|Jersey|TEXT \n",
            "3|POS|TEXT\n",
            "4|AGE|INT \n",
            "5|HT|TEXT \n",
            "6|WT|TEXT \n",
            "7|COLLEGE|TEXT \n",
            "8|SALARY|TEXT eg. \n",
            "\n",
            "\n",
            "Write a sqlite query to answer the following question. Follow instructions exactly\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = make_llama_3_prompt(user, system)"
      ],
      "metadata": {
        "id": "QqrDoRR3jRj1"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(llm.generate(prompt, max_new_tokens=200))"
      ],
      "metadata": {
        "id": "byBjyQ53jefj",
        "outputId": "499844d2-56d3-4d2c-bed4-b0e936ebf56e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To answer this question, we can use the following SQLite query:\n",
            "\n",
            "```sql\n",
            "SELECT NAME, SALARY\n",
            "FROM nba_roster\n",
            "ORDER BY SALARY DESC\n",
            "LIMIT 1;\n",
            "```\n",
            "\n",
            "This query will return the name and salary of the highest paid NBA player. The `ORDER BY SALARY DESC` clause sorts the players by their salary in descending order (highest to lowest), and the `LIMIT 1` clause ensures that we only get the top result.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = llm.generate(prompt, output_type={\"sqlite_query\": \"str\"}, max_new_tokens=200)"
      ],
      "metadata": {
        "id": "ZxpygzOSjh_d"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_sql(result['sqlite_query'], con=engine) # it will show the output in the data frame(query result)"
      ],
      "metadata": {
        "id": "IXoKvIUEknwN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}